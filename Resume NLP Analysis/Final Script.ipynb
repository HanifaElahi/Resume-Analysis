{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "!pip install squarify\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('popular')\n",
    "# Install nltk Dependencies\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('data')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "import os\n",
    "from os import listdir\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from os.path import isfile, join\n",
    "from io import StringIO\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from nltk.util import ngrams\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "with open(r'skills.txt',encoding=\"utf8\") as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for line in content:\n",
    "    tokens=word_tokenize(line)\n",
    "    tok=[w.lower() for w in tokens]\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    strpp=[w.translate(table) for w in tok]\n",
    "    words=[word for word in strpp if word.isalpha()]\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    words=[w for w in words if not w in stop_words]\n",
    "    x.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing words that are not useful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('common.txt') as f:\n",
    "    content2 = f.read()\n",
    "ntexts=[]\n",
    "l=len(texts)\n",
    "for j in range(l):\n",
    "    s=texts[j]\n",
    "    res = [i for i in s if i not in content2]\n",
    "    ntexts.append(res)\n",
    "print(texts[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ntexts))\n",
    "texts=ntexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\"]\n",
    "x=ntexts\n",
    "# Create the relevant phrases from the list of sentences:\n",
    "phrases = Phrases(x, common_terms=common_terms)\n",
    "# The Phraser object is used from now on to transform sentences\n",
    "bigram = Phraser(phrases)\n",
    "# Applying the Phraser to transform our sentences\n",
    "all_sentences = list(bigram[x])\n",
    "model=gensim.models.Word2Vec(all_sentences,size=5000,min_count=2,workers=4,window=4)\n",
    "model.save(\"final.model\")\n",
    "wrds=list(model.wv.vocab)\n",
    "print(len(wrds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=model.wv.most_similar(\"machine_learning\")\n",
    "b=model.wv.most_similar(\"analytics\")\n",
    "c=model.wv.most_similar(\"ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADDING new words in the vocab of pre-trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"final.model\") \n",
    "model.min_count = 1\n",
    "docs = [[\"Oracle\", \"MERN\"]]\n",
    "model.build_vocab(docs, update=True)\n",
    "model.train(docs, total_examples = len(docs), epochs = 10)\n",
    "#model.train([[\"Oracle\", \"MERN\"]], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=model.wv.most_similar(\"Oracle\")\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESUME**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords Section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_stopwords = stopwords.words('english')\n",
    "gensim_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "sw_list = ['summary','year','education','page','summary','college','university','company','skills','bachelor','country',\n",
    "           'address','email','name','phone','house','jan','feb','mar','apr','may','june','july','aug','sep','oct','nov',\n",
    "           'dec','com','gmail','school','hospital']\n",
    "\n",
    "stopwords_list.extend(gensim_stopwords) #appending gensim stopwords in nltk\n",
    "stopwords_list.extend(spacy_stopwords) #appending spacy stopwords in nltk\n",
    "stopwords_list.extend(sw_list) #appending customized stopwords in nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for cleaning data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(file_content):\n",
    "\n",
    "    file_content = file_content.lower()\n",
    "    file_content = re.sub('\\[.*?\\]', '', file_content)\n",
    "    file_content = re.sub('[‘’“”…]', '', file_content)\n",
    "    file_content = re.sub('\\n', '', file_content) \n",
    "    file_content = re.sub('\\t\\t', '', file_content)\n",
    "    file_content = re.sub('[%s]' % re.escape(string.punctuation), '', file_content)\n",
    "    file_content = re.sub('\\w*\\d\\w*', '', file_content)\n",
    "    file_content = re.sub('\\W', ' ', file_content)\n",
    "    file_content = remove_stopwords(file_content)\n",
    "    tokens = word_tokenize(file_content)\n",
    "    tokens_without_sw = [word for word in tokens if not word in stopwords_list]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in tokens_without_sw])\n",
    "    s = list(str(lemmatized_output))\n",
    "    r  = ''.join([lemmatizer.lemmatize(r) for r in s])\n",
    "    tokens2 = word_tokenize(r)\n",
    "    return tokens2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Function for extracting email**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email(text):\n",
    "    email = re.compile(r'[a-zA-Z0-9-\\.]+@[a-zA-Z-\\.]*\\.(com|edu|net)')\n",
    "    matches = email.finditer(text)\n",
    "    for match in matches:\n",
    "        return(match.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for extracting Phone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phone(text):\n",
    "    phone = re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?')\n",
    "    matches = phone.finditer(text)\n",
    "    for match in matches:\n",
    "        return(match.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for extracting Name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    matcher.add('NAME', [pattern]) #matching\n",
    "    matches = matcher(nlp_text)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for extracting technical skills**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    data = pd.read_csv(\"skills.csv\") \n",
    "    skills = list(data.columns.values)\n",
    "    skillset = []\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for extracting education**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII',\n",
    "            'Matriculation','BSC','PhD'\n",
    "        ]\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for extracting analytical skills**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_analytical_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"AnalyticalSkills.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    a_skills = list(data.columns.values)\n",
    "    \n",
    "    a_skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in a_skills:\n",
    "            a_skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in a_skillset])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for extracting Leadership Skills**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_leadership_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"LeadershipSkills.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    l_skills = list(data.columns.values)\n",
    "    \n",
    "    l_skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in l_skills:\n",
    "            l_skillset.append(token)\n",
    "\n",
    "    return [i.capitalize() for i in set([i.lower() for i in l_skillset])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for extracting Teamwork skills**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_teamwork_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"TeamworkSkills.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    t_skills = list(data.columns.values)\n",
    "    \n",
    "    t_skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in t_skills:\n",
    "            t_skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in t_skillset])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for extracting Communication skills**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_communication_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"CommunicationSkills.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    c_skills = list(data.columns.values)\n",
    "    \n",
    "    c_skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in c_skills:\n",
    "            c_skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in c_skillset])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for extracting action verbs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_action_verbs(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"Action_words.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    c_skills = list(data.columns.values)\n",
    "    \n",
    "    c_skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in c_skills:\n",
    "            c_skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in c_skillset])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for extracting measureable results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_measureable_results(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"Measurable_results.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    c_skills = list(data.columns.values)\n",
    "    \n",
    "    c_skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in c_skills:\n",
    "            c_skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in c_skillset])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for extracting result-driven skills**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_result_driven_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"Result_driven_skills.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    c_skills = list(data.columns.values)\n",
    "    \n",
    "    c_skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in c_skills:\n",
    "            c_skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in c_skillset])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for extracting Strengths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_strength(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"combined.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    strengths = list(data.columns.values)\n",
    "    \n",
    "    strengthsset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in strengths:\n",
    "            strengthsset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in strengthsset])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for cleaning through nlp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_cleaning(content, file_content):\n",
    "    digits = []\n",
    "    name_nlp = []\n",
    "    loc = []\n",
    "    date = []\n",
    "    norp = []\n",
    "    law = []\n",
    "    percent = []\n",
    "    product = []\n",
    "    time = []\n",
    "    x = []\n",
    "    \n",
    "    doc = nlp(content)\n",
    "    for ent in doc.ents:\n",
    "    #print(f'Entity:{ent}, Label: {ent.label_}')\n",
    "        if 'ORG' in ent.label_:\n",
    "            x.append(ent)\n",
    "        if 'CARDINAL' in ent.label_:\n",
    "            digits.append(ent)\n",
    "        if 'PERSON' in ent.label_:\n",
    "            name_nlp.append(ent)\n",
    "        if 'GPE' in ent.label_:\n",
    "            loc.append(ent)\n",
    "        if 'DATE' in ent.label_:\n",
    "            date.append(ent)\n",
    "        if 'NORP' in ent.label_:\n",
    "            norp.append(ent)\n",
    "        if 'LAW' in ent.label_:\n",
    "            law.append(ent)\n",
    "        if 'PERCENT' in ent.label_:\n",
    "            percent.append(ent)\n",
    "        if 'PRODUCT' in ent.label_:\n",
    "            product.append(ent)\n",
    "        if 'TIME' in ent.label_:\n",
    "            time.append(ent)\n",
    "    x = str(x)\n",
    "    x_tokens = word_tokenize(x)\n",
    "    date = str(date)\n",
    "    date_tokens = word_tokenize(date)\n",
    "    loc = str(loc)\n",
    "    loc_tokens = word_tokenize(loc)\n",
    "    digits = str(digits)\n",
    "    digits_tokens = word_tokenize(digits)\n",
    "    norp = str(norp)\n",
    "    norp_tokens = word_tokenize(norp)\n",
    "    law = str(law)\n",
    "    law_tokens = word_tokenize(law)\n",
    "    percent = str(percent)\n",
    "    percent_tokens = word_tokenize(percent)\n",
    "    product = str(product)\n",
    "    product_tokens = word_tokenize(product)\n",
    "    time = str(time)\n",
    "    time_tokens = word_tokenize(time)\n",
    "    \n",
    "    mylist = [word for word in file_content if word not in date_tokens and not word in x_tokens and not word in loc_tokens and not word in digits_tokens \n",
    "              and word not in norp_tokens and not word in law_tokens and not word in percent_tokens and not word in \n",
    "              product_tokens and word not in time_tokens]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final = ' '.join([lemmatizer.lemmatize(w) for w in mylist])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for creating wordcloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(final):\n",
    "    wordcloud = WordCloud(max_font_size=35, max_words=100, background_color=\"white\").generate(final)\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for creating wordcloud of skills**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud_of_skills(skills):\n",
    "    cloud = WordCloud(max_font_size=80,colormap=\"hsv\").generate(skills)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for Experience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_finder(c):\n",
    "    all_ex=[]\n",
    "    month=[\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "    for i in c:\n",
    "        start_date=i[0].split()\n",
    "        \n",
    "        if start_date[0] in month:\n",
    "            index=month.index(start_date[0])\n",
    "            index+=1\n",
    "        start_date = datetime(int(start_date[-1]),index,1,0, 0, 0)\n",
    "        end_date=i[3].split()\n",
    "        \n",
    "        if end_date[0] in month:\n",
    "            index=month.index(end_date[0])\n",
    "            index+=1\n",
    "        \n",
    "        end_date = datetime(int(end_date[-1]),index,1,0, 0, 0)\n",
    "        experince=end_date-start_date\n",
    "        if float(str(experince).split(\",\")[0].split()[0])<365:\n",
    "            experince=float(str(experince).split(\",\")[0].split()[0])\n",
    "            experince=experince/365\n",
    "            experince*=12\n",
    "            experince=str(experince)+ \" months\"\n",
    "        elif float(str(experince).split(\",\")[0].split()[0])>=365:    \n",
    "            experince=experince/365\n",
    "            experince=str(experince).split(\",\")[0].split()[0]+ \" years\"\n",
    "        all_ex.append(experince)\n",
    "    \n",
    "    return all_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading Reseume from directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath=r'Resumes'\n",
    "#Path for the files\n",
    "onlyfiles = [os.path.join(mypath, f) for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for creating Personal Deatils csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Name','Email','Phone','Education','Expereince','Technical Skills','Analytical Skills','Leadership Skills',\n",
    "          'Teamwork Skills','Communication Skills','Action verbs']    \n",
    "\n",
    "with open('Personal Details.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for creating csv for Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Name','Expereince','Analytical Skills','Leadership Skills','Teamwork Skills','Communication Skills','Action verbs',\n",
    "          'Measureable Results','Result-driven Skills','Strengths','Soft Skills']    \n",
    "\n",
    "with open('Details.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = []\n",
    "list1 = []\n",
    "\n",
    "\n",
    "def main(file):\n",
    "    txt_file = open(file,'rt',encoding = 'utf-8')\n",
    "    content = txt_file.read()\n",
    "    file_content = content\n",
    "    #######################################################################\n",
    "    date = []\n",
    "    doc = nlp(content)\n",
    "    exp = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if 'DATE' in ent.label_:\n",
    "            date.append(ent)\n",
    "    m = str(date)\n",
    "    try: \n",
    "        c = re.findall(r\"(?P<fmonth>\\w+(?P<fh>.)\\d+)\\s*(\\s|-|to)\\s*(?P<smonth>\\w+(?P<sh>.)\\d+|present|date|now)\",m)\n",
    "        all_experience=experience_finder(c)\n",
    "        count=0\n",
    "        for x in all_experience:\n",
    "            if \"years\" in x:\n",
    "                x=float(x.split()[0])\n",
    "                count=count+x\n",
    "            elif 'months' in x:\n",
    "                x=float(x.split()[0])/12\n",
    "                count=count+x\n",
    "                exp = count\n",
    "\n",
    "    except:\n",
    "        exp = 0\n",
    "    ######################################################################\n",
    "    \n",
    "    final_list = [] \n",
    "    details = []\n",
    "    \n",
    "    name = extract_name(file_content)\n",
    "    print(\"Name : \",name)\n",
    "    \n",
    "    email = extract_email(file_content)\n",
    "    print(\"\\nEmail : \",email)\n",
    "    \n",
    "    phone = extract_phone(file_content)\n",
    "    print(\"\\nPhone : \",phone)\n",
    "    \n",
    "    technical_skills = extract_skills(file_content)\n",
    "    print(\"\\nTechincal Skills : \",technical_skills)\n",
    "    \n",
    "    analytical_skills = extract_analytical_skills(file_content)\n",
    "    a = len(analytical_skills)\n",
    "    print(\"\\nAnalytical Skills : \",analytical_skills)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        listToStr = ' '.join([str(elem) for elem in analytical_skills])\n",
    "        print(\"\\n**************************************** WORLCLOUD FOR ANALYTICAL SKILLS ****************************************\")\n",
    "        wordcloud = create_wordcloud(listToStr)\n",
    "    except:\n",
    "        print(\"No analytical skill\")\n",
    "    \n",
    "    leadership_skills = extract_leadership_skills(file_content)\n",
    "    b = len(leadership_skills)\n",
    "    print(\" \\nLeadership Skills : \",leadership_skills)\n",
    "    \n",
    "    try: \n",
    "        listToStr = ' '.join([str(elem) for elem in leadership_skills])\n",
    "        print(\"\\n**************************************** WORLCLOUD FOR LEADERSHIP SKILLS ****************************************\")\n",
    "        wordcloud = create_wordcloud(listToStr)\n",
    "    except:\n",
    "        print(\"No leadership skill\")\n",
    "    \n",
    "    teamwork_skills = extract_teamwork_skills(file_content)\n",
    "    c = len(teamwork_skills)\n",
    "    print(\" \\nTeamwork Skills : \",teamwork_skills)\n",
    "    \n",
    "    try: \n",
    "        listToStr = ' '.join([str(elem) for elem in teamwork_skills])\n",
    "        print(\"\\n**************************************** WORLCLOUD FOR TEAMWORK SKILLS ****************************************\")\n",
    "        wordcloud = create_wordcloud(listToStr)\n",
    "    except:\n",
    "        print(\"No Teamwork skill\")\n",
    "    \n",
    "    communication_skills = extract_communication_skills(file_content)\n",
    "    d = len(communication_skills)\n",
    "    print(\" \\nCommunication Skills : \",communication_skills)\n",
    "    \n",
    "    try: \n",
    "        listToStr = ' '.join([str(elem) for elem in communication_skills])\n",
    "        print(\"\\n***************************************** WORLCLOUD FOR COMMUNICATION SKILLS ****************************************\")\n",
    "        wordcloud = create_wordcloud(listToStr)\n",
    "    except:\n",
    "        print(\"No communication skill\")\n",
    "    \n",
    "    action_verbs = extract_action_verbs(file_content)\n",
    "    e = len(action_verbs)\n",
    "    print(\" \\nAction Verbs :  \",action_verbs)\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        listToStr = ' '.join([str(elem) for elem in action_verbs])\n",
    "        print(\"\\n***************************************** WORLCLOUD FOR ACTION VERBS ****************************************\")\n",
    "        wordcloud = create_wordcloud(listToStr)\n",
    "    except:\n",
    "        print(\"No action verbs\")\n",
    "    \n",
    "    measureable_result = extract_measureable_results(file_content)\n",
    "    f = len(measureable_result)\n",
    "    print(\" \\nMeasureable Results :  \",measureable_result)\n",
    "    try:\n",
    "        \n",
    "        listToStr = ' '.join([str(elem) for elem in measureable_result])\n",
    "        print(\"\\n**************************************** WORLCLOUD FOR MEASUREABLE RESULT ****************************************\")\n",
    "        wordcloud = create_wordcloud(listToStr)\n",
    "    except:\n",
    "        print(\"No measureable result\")\n",
    "    \n",
    "    result_driven_skills = extract_result_driven_skills(file_content)\n",
    "    g = len(result_driven_skills)\n",
    "    print(\" \\nResult driven skills :  \",result_driven_skills)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        listToStr = ' '.join([str(elem) for elem in result_driven_skills])\n",
    "        print(\"\\n**************************************** WORLCLOUD FOR RESULT DRIVEN SKILLS ****************************************\")\n",
    "        wordcloud = create_wordcloud(listToStr)\n",
    "    except:\n",
    "        print(\"No result driven skill\")\n",
    "    \n",
    "    strength = extract_strength(file_content)\n",
    "    print(\" \\nStrength : \",strength)\n",
    "    \n",
    "    education = extract_education(file_content)\n",
    "    print(\" \\nEducation : \",education)\n",
    "    \n",
    "    print(\"\\nExperience\",exp)\n",
    "    \n",
    "    strengths_skills = strength + technical_skills\n",
    "    h = len(strengths_skills)\n",
    "    \n",
    "    soft_skills = analytical_skills + leadership_skills + teamwork_skills + communication_skills + action_verbs + measureable_result + result_driven_skills  \n",
    "    i = len(soft_skills)\n",
    "    \n",
    "    final_list.append(name)\n",
    "    final_list.append(email)\n",
    "    final_list.append(phone)\n",
    "    final_list.append(education)\n",
    "    final_list.append(exp)\n",
    "    final_list.append(technical_skills)\n",
    "    final_list.append(analytical_skills)\n",
    "    final_list.append(leadership_skills)\n",
    "    final_list.append(teamwork_skills)\n",
    "    final_list.append(communication_skills)\n",
    "    final_list.append(action_verbs)\n",
    "    \n",
    "    details.append(name)\n",
    "    details.append(exp)\n",
    "    details.append(a)\n",
    "    details.append(b)\n",
    "    details.append(c)\n",
    "    details.append(d)\n",
    "    details.append(e)\n",
    "    details.append(f)\n",
    "    details.append(g)\n",
    "    details.append(h)\n",
    "    details.append(i)\n",
    "\n",
    "   \n",
    "    with open(r'Personal Details.csv', 'a+', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(final_list)\n",
    "    \n",
    "    with open(r'Details.csv', 'a+', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(details)\n",
    "    \n",
    "    file_content = clean_data(file_content)\n",
    "    file_content = nlp_cleaning(content,file_content)\n",
    "\n",
    "    \n",
    "    print(\"\\n***************************************** WORLCLOUD FOR RESUME ****************************************\")\n",
    "    wordcloud = create_wordcloud(file_content)\n",
    "    list1 = [file_content]\n",
    "    \n",
    "    print(\"\\n**************************************** WORLCLOUD FOR STRENGTH ****************************************\")\n",
    "    listToStr = ' '.join([str(elem) for elem in strengths_skills])\n",
    "    wordcloud = create_wordcloud(listToStr)\n",
    "    \n",
    "    print(\"\\n**************************************** WORLCLOUD FOR SOFT SKILLS ****************************************\")\n",
    "    listToStr = ' '.join([str(elem) for elem in soft_skills])\n",
    "    try : \n",
    "        wordcloud = create_wordcloud_of_skills(listToStr)\n",
    "    except:\n",
    "        print(\" \")\n",
    "    \n",
    "    print(\"\\n******************************************* SOFT SKILLS CLASSIFICATION **************************************\\n\")\n",
    "    \n",
    "    labels = ['Analytical Skills','Leadership Skills','Teamwork Skills','Communication Skills']\n",
    "    data1 = [a,b,c,d]\n",
    "    colors = ['lightskyblue','greenyellow','hotpink','gold']\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    plt.pie(data1, colors = colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85)\n",
    "    #draw circle\n",
    "    centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "    # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    ax1.axis('equal')  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n******************************************* SOFT SKILLS vs TECHNICAL SKILLS **************************************\\n\")\n",
    "    p = len(soft_skills)\n",
    "    q = len(strengths_skills)\n",
    "    labels = ['Soft Skills','Technical Skills']\n",
    "    data2 = [p,q]\n",
    "    colors = ['lightskyblue','greenyellow']\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    plt.pie(data2, colors = colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85)\n",
    "    #draw circle\n",
    "    centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "    # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    ax1.axis('equal')  \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING PROFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_profile(file):\n",
    "    model=Word2Vec.load(\"final.model\")\n",
    "    txt_file = open(file,'rt',encoding = 'utf-8')\n",
    "    text = txt_file.read()\n",
    "    text = text.lower()\n",
    "    \n",
    "    keywords = ['statistics','language','machine_learning','deep','artificial_intelligence','python','data']\n",
    "    \n",
    "    for i in range(len(keywords)):\n",
    "        word=keywords[i]\n",
    "        x = [nlp(text[0]) for text in model.wv.most_similar(word)]\n",
    "\n",
    "    print(\"*******************************************\")\n",
    "    \n",
    "    #print(stats_words,NLP_words)\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    matcher.add('KEYWORD', None, *x)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    d = []  \n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        rule_id = nlp.vocab.strings[match_id]  # get the unicode I\n",
    "        span = doc[start : end]               # get the matched slice of the doc\n",
    "        d.append((rule_id, span.text))      \n",
    "    keywords = \"\\n\".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items())\n",
    "    print(\"KEYWORDS\")\n",
    "    print(keywords)\n",
    "    \n",
    "    ## convertimg string of keywords to dataframe\n",
    "    df = pd.read_csv(StringIO(keywords),names = ['Keywords_List'])\n",
    "    df1 = pd.DataFrame(df.Keywords_List.str.split(' ',1).tolist(),columns = ['Subject','Keyword'])\n",
    "    df2 = pd.DataFrame(df1.Keyword.str.split('(',1).tolist(),columns = ['Keyword', 'Count'])\n",
    "    df3 = pd.concat([df1['Subject'],df2['Keyword'], df2['Count']], axis =1) \n",
    "    df3['Count'] = df3['Count'].apply(lambda x: x.rstrip(\")\"))\n",
    "    print(\"********************DF********************\")\n",
    "    print(df)\n",
    "    \n",
    "    base = os.path.basename(file)\n",
    "    filename = os.path.splitext(base)[0]\n",
    "    \n",
    "       \n",
    "    name = filename.split('_')\n",
    "    print(name)\n",
    "    name2 = name[0]\n",
    "    name2 = name2.lower()\n",
    "    ## converting str to dataframe\n",
    "    name3 = pd.read_csv(StringIO(name2),names = ['Candidate Name'])\n",
    "    \n",
    "    dataf = pd.concat([name3['Candidate Name'], df3['Subject'], df3['Keyword'], df3['Count']], axis = 1)\n",
    "    dataf['Candidate Name'].fillna(dataf['Candidate Name'].iloc[0], inplace = True)\n",
    "    print(\"******************DATAF**************\")\n",
    "    print(dataf)\n",
    "\n",
    "    return(dataf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to execute the above functions \n",
    "final_db=pd.DataFrame()\n",
    "i=0\n",
    "while i < len(onlyfiles):\n",
    "    file=onlyfiles[i]\n",
    "    data=main(file)\n",
    "    print(data)\n",
    "    dat=create_profile(file)\n",
    "    final_db=final_db.append(dat)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to count words under each category and visualize it through MAtplotlib\n",
    "final_db2 = final_db['Keyword'].groupby([final_db['Candidate Name'], final_db['Subject']]).count().unstack()\n",
    "final_db2.reset_index(inplace = True)\n",
    "final_db2.fillna(0,inplace=True)\n",
    "candidate_data = final_db2.iloc[:,1:]\n",
    "candidate_data.index = final_db2['Candidate Name']\n",
    "#the candidate profile in a csv format\n",
    "cand=candidate_data.to_csv('candidate_profile.csv')\n",
    "cand_profile=pd.read_csv('candidate_profile.csv')\n",
    "cand_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "ax = candidate_data.plot.barh(title=\"Keywords in Resume according to category\", legend=False, figsize=(50,60),stacked=True)\n",
    "skills = []\n",
    "for j in candidate_data.columns:\n",
    "    for i in candidate_data.index:\n",
    "        skill = str(j)+\": \" + str(candidate_data.loc[i][j])\n",
    "        skills.append(skill)\n",
    "patches = ax.patches\n",
    "for skill, rect in zip(skills, patches):\n",
    "    width = rect.get_width()\n",
    "    if width > 0:\n",
    "        x = rect.get_x()\n",
    "        y = rect.get_y()\n",
    "        height = rect.get_height()\n",
    "        ax.text(x + width/2., y + height/2., skill, ha='center', va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_db['Keyword'].groupby([final_db['Candidate Name'], final_db['Subject']]).count().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    final_db\n",
    "    .groupby(final_db['Candidate Name'])\n",
    "    .Subject\n",
    "    .value_counts()\n",
    "    .unstack()\n",
    "    .fillna(0)\n",
    ")\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=1800)\n",
    "sns.heatmap(data,linewidths=.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZING SKILLS OF EMLOYEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df2 = pd.read_csv(\"Details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(['Expereince','Soft Skills'],inplace = True,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red','gold','lightskyblue','hotpink','greenyellow','orange','yellow','lightcoral']\n",
    "df2.plot(\n",
    "  x = 'Name', \n",
    "  kind = 'barh', \n",
    "  color = colors,\n",
    "  stacked = True, \n",
    "  figsize = (25,35),\n",
    "  title = 'Soft Skills Percentage of Employees', \n",
    "  mark_right = True)\n",
    "\n",
    "df_total = df2['Strengths'] + df2[\"Analytical Skills\"] + df2[\"Leadership Skills\"] + df2[\"Teamwork Skills\"] + df2[\"Communication Skills\"] + df2[\"Action verbs\"] + df2[\"Measureable Results\"] + df2[\"Result-driven Skills\"]\n",
    "df_rel = df2[df2.columns[1:]].div(df_total, 0)*100\n",
    "\n",
    "for n in df_rel:\n",
    "    for i, (cs, ab, pc) in enumerate(zip(df2.iloc[:, 1:].cumsum(1)[n], df2[n], df_rel[n])):\n",
    "\n",
    "        plt.text(cs - ab / 2, i, str(np.round(pc, 1)) + '%', va = 'center', ha = 'center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARING SOFT SKILLS vs TECHNICAL SKILLS of EMPLOYEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"Details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(['Expereince','Analytical Skills','Leadership Skills','Communication Skills','Teamwork Skills','Action verbs','Measureable Results','Result-driven Skills'],inplace = True,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['lightskyblue','hotpink']\n",
    "df3.plot(\n",
    "  x = 'Name', \n",
    "  kind = 'barh', \n",
    "  color = colors,\n",
    "  stacked = True, \n",
    "  figsize = (25,35),\n",
    "  title = 'Soft Skills vs Technical Strengths of Employees', \n",
    "  mark_right = True)\n",
    "\n",
    "df_total = df3['Strengths'] + df3[\"Soft Skills\"]\n",
    "df_rel = df3[df3.columns[1:]].div(df_total, 0)*100\n",
    "\n",
    "for n in df_rel:\n",
    "    for i, (cs, ab, pc) in enumerate(zip(df3.iloc[:, 1:].cumsum(1)[n], df3[n], df_rel[n])):\n",
    "\n",
    "        plt.text(cs - ab / 2, i, str(np.round(pc, 1)) + '%', va = 'center', ha = 'center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDUCATION VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"Personal Details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute percentages from csv\n",
    "degrees = set(x['Education'])\n",
    "percentages = []\n",
    "for degree in degrees:\n",
    "    percentages.append(x[x['Education'] == degree].shape[0])\n",
    "percentages = np.array(percentages)\n",
    "percentages = ((percentages / percentages.sum()) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for tree map\n",
    "labels = [degree + '\\n({0:.1f}%)'.format(percentage) for degree, percentage in zip(degrees, percentages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import squarify\n",
    "sns.set()\n",
    "# Create figure\n",
    "plt.figure(figsize=(25, 25), dpi=500)\n",
    "squarify.plot(percentages, label=labels, color=sns.color_palette('colorblind', len(degrees)))\n",
    "plt.axis('off')\n",
    "# Add title\n",
    "plt.title('Degrees')\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKILLS SCORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"Details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_analyticalskills = df2[df2.columns[2]].tolist()\n",
    "group_leadershipskills = df2[df2.columns[3]].tolist()\n",
    "group_teamworkskills = df2[df2.columns[4]].tolist()\n",
    "group_communicationskills = df2[df2.columns[5]].tolist()\n",
    "group_actionverbs = df2[df2.columns[6]].tolist()\n",
    "group_measureableresults = df2[df2.columns[7]].tolist()\n",
    "group_resultdrivenskills = df2[df2.columns[8]].tolist()\n",
    "group_strengths = df2[df2.columns[9]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Skills': ['Analytical'] * len(group_analyticalskills) + ['Leadership'] * len(group_leadershipskills)\n",
    "                    + ['Teamwork'] * len(group_teamworkskills) + ['Communication'] * len(group_communicationskills) + \n",
    "                    ['Action verbs'] * len(group_actionverbs) + ['Measureable Results'] * len(group_measureableresults)\n",
    "                    + ['Result-driven'] * len(group_resultdrivenskills) + ['Technical'] * len(group_strengths),\n",
    "                    'Skills score': group_analyticalskills + group_leadershipskills + group_teamworkskills + group_communicationskills + \n",
    "                     group_actionverbs + group_measureableresults + group_resultdrivenskills + group_strengths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "plt.figure(dpi=250)\n",
    "plt.rc('xtick', labelsize=5)\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "# Create boxplot\n",
    "sns.violinplot('Skills', 'Skills score', data=data)\n",
    "# Despine\n",
    "sns.despine(left=True, right=True, top=True)\n",
    "# Add title\n",
    "plt.title('Skills scores')\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMPLOYEES EXPERIENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"Details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Expereince'].replace({\"[]\": \"0\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df3['Name'].tolist()\n",
    "n = df3['Expereince'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize = (10, 15))\n",
    "plt.barh(m,n,color = 'purple')\n",
    "plt.xlabel('Expereince')\n",
    "plt.ylabel('Candidate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df4 = pd.read_csv(\"Details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df4['Soft Skills'] + df4['Strengths']\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create figure\n",
    "plt.figure(figsize=(6, 4), dpi=150)\n",
    "# Create histogram\n",
    "plt.hist(p, bins=10)\n",
    "plt.axvline(x=50, color='r')\n",
    "plt.axvline(x=100, color='r', linestyle= '--')\n",
    "plt.axvline(x=150, color='r', linestyle= '--')\n",
    "plt.axvline(x=200, color='r', linestyle= '--')\n",
    "# Add labels and title\n",
    "plt.xlabel('Skill score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Skills scores for candidates')\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
